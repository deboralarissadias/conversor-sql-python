# SQL to Spark Converter - Project Summary

## Overview
A comprehensive Python tool developed for the hackathon (Squad 06) that automatically converts Oracle/PostgreSQL SQL queries to Apache Spark code, supporting both PySpark DataFrame API and SparkSQL formats.

## ðŸŽ¯ Project Goals
- **Primary Goal**: Convert legacy SQL queries to modern Apache Spark code
- **Target Users**: Data engineers migrating from traditional databases to Spark
- **Key Value**: Accelerate migration process and reduce manual conversion errors

## ðŸ—ï¸ Architecture

### Core Components
```
sql_to_spark_converter/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py              # Main converter class
â”‚   â”œâ”€â”€ parser/
â”‚   â”‚   â”œâ”€â”€ __init__.py          # Parser module exports
â”‚   â”‚   â””â”€â”€ sql_parser.py        # SQL parsing logic using sqlparse
â”‚   â””â”€â”€ generators/
â”‚       â”œâ”€â”€ __init__.py          # Generator module exports
â”‚       â”œâ”€â”€ pyspark_generator.py # PySpark DataFrame API generator
â”‚       â””â”€â”€ sparksql_generator.py# SparkSQL code generator
â”œâ”€â”€ main.py                      # CLI interface
â”œâ”€â”€ examples/                    # Sample SQL files
â”œâ”€â”€ test_converter.py           # Test suite
â””â”€â”€ demo.py                     # Interactive demonstration
```

## ðŸ”§ Technical Implementation

### SQL Parser (`sql_parser.py`)
- Uses `sqlparse` library for robust SQL parsing
- Extracts key components: SELECT, FROM, WHERE, JOIN, GROUP BY, HAVING, ORDER BY
- Handles complex queries with subqueries, window functions, and aggregations
- Identifies Oracle/PostgreSQL specific functions for conversion

### PySpark Generator (`pyspark_generator.py`)
- Converts parsed SQL to PySpark DataFrame API calls
- Generates complete Python code with proper imports
- Handles JOIN operations, filtering, grouping, and ordering
- Produces executable PySpark code

### SparkSQL Generator (`sparksql_generator.py`)
- Converts Oracle/PostgreSQL SQL to Spark-compatible SQL
- Function mapping (NVL â†’ COALESCE, SUBSTR â†’ SUBSTRING, etc.)
- Date literal conversion and ROWNUM handling
- Maintains SQL readability with proper formatting

## ðŸš€ Key Features

### âœ… Supported SQL Features
- **SELECT statements** with complex projections
- **JOINs** (INNER, LEFT, RIGHT, FULL OUTER)
- **WHERE clauses** with complex conditions
- **GROUP BY and HAVING** clauses
- **ORDER BY** with multiple columns and ASC/DESC
- **Aggregate functions** (COUNT, SUM, AVG, MIN, MAX)
- **Window functions** (ROW_NUMBER, RANK, etc.)
- **CASE statements** and conditional logic
- **Subqueries** and Common Table Expressions (CTEs)

### ðŸ”„ Oracle/PostgreSQL Function Conversion
- `NVL` â†’ `COALESCE`
- `SUBSTR` â†’ `SUBSTRING`
- `SYSDATE` â†’ `CURRENT_TIMESTAMP()`
- `ROWNUM` â†’ `ROW_NUMBER() OVER (ORDER BY 1)`
- Date literals: `DATE 'YYYY-MM-DD'` â†’ `DATE('YYYY-MM-DD')`

## ðŸ’» Usage Examples

### CLI Usage
```bash
# Convert from file to both formats
python main.py -f query.sql --output both

# Convert query string to PySpark only
python main.py -q "SELECT * FROM users WHERE age > 25" --output pyspark

# Save output to file
python main.py -f complex_query.sql --output both -o converted_code.py
```

### Programmatic Usage
```python
from src import SQLToSparkConverter

converter = SQLToSparkConverter()

# Generate PySpark DataFrame API code
pyspark_code = converter.to_pyspark(sql_query)

# Generate SparkSQL code
sparksql_code = converter.to_sparksql(
